{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71cc5ee",
   "metadata": {},
   "source": [
    "# lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09e666da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.logger import configure\n",
    "import stable_baselines3 as sb3\n",
    "import sb3_contrib  as sb3_trib\n",
    "from sb3_contrib import TRPO\n",
    "from stable_baselines3.common.callbacks import StopTrainingOnRewardThreshold, EvalCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor # Import the Monitor wrapper\n",
    "import os\n",
    "import time\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2530551a",
   "metadata": {},
   "source": [
    "# preset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a860a95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "soc_preset = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17300fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "Action_arr = pd.DataFrame(columns=[\"Ge\",\"Re\",'l1','REavai','Ebess','EbessMAX','EbessMIN','soc'])\n",
    "df_each_step = pd.DataFrame(columns=[\"step\", \"action\", \"state\", \"total_reward\", \"lamda\", \"soc\"])\n",
    "df_each_episode = pd.DataFrame(columns=[\"Ge_ar\", \"Re_ar\", \"Be_ar\", \"soc\", \"load\", \"reward\"])\n",
    "\n",
    "load_standard = np.array([439,417,394,376,386,353,339,351,385,422,439,443,458,453,427,394,370,369,415,478,460,466,480,457], dtype=np.float32)\n",
    "PVAvai_standard = np.array([0,0,0,0,0,13.2,48.9,104,177,251,306,313,315,279,240,156,64.3,37.4,0,0,0,0,0,0], dtype=np.float32)\n",
    "WindAvai_standard =np.array([149,153,151,152,151,147,150,149,150,149,151,154,157,157,158,154,159,156,157,157,162,158,158,162])*2\n",
    "ReAvai_standard = PVAvai_standard + WindAvai_standard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b508f127",
   "metadata": {},
   "source": [
    "# env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e25e2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_value(x, in_min, in_max, out_min, out_max):\n",
    "    # Công thức chuyển đổi giá trị\n",
    "    return (x - in_min) / (in_max - in_min) * (out_max - out_min) + out_min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de50bae",
   "metadata": {},
   "source": [
    "## trainng env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d6faca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimizeProductEnv_24_4(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(MinimizeProductEnv_24_4, self).__init__()\n",
    "        # Các giá trị của hệ thống\n",
    "        self.max_steps = 24\n",
    "        self.load_standard = load_standard\n",
    "        self.ReAvai_standard = ReAvai_standard\n",
    "        self.noise_percentage = 0.1\n",
    "        self.load = None\n",
    "        self.ReAvai = None\n",
    "        self.soc_prev = soc_preset\n",
    "        self.cap_max = 2000.0\n",
    "        self.P_bess_max = 550.0\n",
    "\n",
    "        # Obs: soc\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([-10, 0, 0, 0], dtype=np.float32),\n",
    "            high=np.array([10, np.inf, np.inf, self.max_steps], dtype=np.float32)\n",
    "        )\n",
    "\n",
    "        # action  Gen, RE\n",
    "        self.action_space =  spaces.Box(low=np.array([-1,-1], dtype=np.float32),\n",
    "                                     high=np.array([1,1], dtype=np.float32))\n",
    "        self.total_reward = 0\n",
    "        self.Ge_ar = []\n",
    "        self.Re_ar = []\n",
    "        self.Be_ar = []\n",
    "        self.soc = []\n",
    "        self.load_ = []\n",
    "\n",
    "    def _add_percentage_noise(self, data, percentage=0.1):\n",
    "        lower_bound = 1 - percentage\n",
    "        upper_bound = 1 + percentage\n",
    "        factors = np.random.uniform(lower_bound, upper_bound, data.shape)\n",
    "        noisy_data = data * factors\n",
    "        return noisy_data\n",
    "\n",
    "    def reset(self, seed=None, options=None, currStep=0, loadForecast=None, REforecast=None, pastRW=0, soc_preset=soc_preset):\n",
    "        super().reset(seed=seed)\n",
    "        self.step_count = currStep\n",
    "        self.total_reward = pastRW\n",
    "        # Tạo bộ dữ liệu load và ReAvai mới với nhiễu cho mỗi episode\n",
    "        self.load = self._add_percentage_noise(self.load_standard, self.noise_percentage)\n",
    "        self.ReAvai = self._add_percentage_noise(self.ReAvai_standard, self.noise_percentage)\n",
    "        self.soc_prev = soc_preset\n",
    "        self.Ge_ar = []\n",
    "        self.Re_ar = []\n",
    "        self.Be_ar = []\n",
    "        self.soc = []\n",
    "        self.load_ = []\n",
    "        self.state = np.array([\n",
    "            self.soc_prev,\n",
    "            self.load_standard[currStep],\n",
    "            self.ReAvai_standard[currStep],\n",
    "            float(currStep)\n",
    "        ], dtype=np.float32)\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        reward = 0\n",
    "        # Cập nhật trạng thái với action\n",
    "        Egen, Ere = action\n",
    "\n",
    "        _max = self.ReAvai[self.step_count] # Sử dụng giá trị ReAvai đã bị nhiễu cho bước hiện tại\n",
    "\n",
    "        Ere=map_value(Ere,-1,1,0,_max)\n",
    "        Ebess_max=min(self.P_bess_max,self.soc_prev*self.cap_max)\n",
    "        Ebess_min=max(-self.P_bess_max,-(1-self.soc_prev)*self.cap_max)\n",
    "        Egen= map_value(Egen,-1,1,0,2000) # Sử dụng giá trị load đã bị nhiễu cho bước hiện tại\n",
    "\n",
    "\n",
    "        #tính Ebess\n",
    "        Ebess=self.load[self.step_count]-Egen-Ere\n",
    "        soc = self.soc_prev-Ebess/self.cap_max\n",
    "        reward = -(\n",
    "            Egen\n",
    "            -(Ere-self.ReAvai[self.step_count])\n",
    "            + 1e6 * ( max(0, Ebess - Ebess_max) + max(0, Ebess_min - Ebess) )\n",
    "            )\n",
    "        self.reward = reward\n",
    "        self.soc_prev=soc\n",
    "\n",
    "        self.total_reward += (reward)\n",
    "\n",
    "        self.step_count += 1\n",
    "\n",
    "        if self.step_count >= self.max_steps:\n",
    "            self.state = np.array([\n",
    "            soc,                                     # Updated SOC\n",
    "            0,             # Current load (vẫn là giá trị bị nhiễu của episode này)\n",
    "            0,           # Available renewable energy (vẫn là giá trị bị nhiễu của episode này)\n",
    "            float(self.step_count)                  # Current time step\n",
    "            ], dtype=np.float32)\n",
    "        else:\n",
    "            self.state = np.array([\n",
    "              soc,                                     # Updated SOC\n",
    "              self.load_standard[self.step_count],             # Current load (vẫn là giá trị bị nhiễu của episode này)\n",
    "              self.ReAvai_standard[self.step_count],           # Available renewable energy (vẫn là giá trị bị nhiễu của episode này)\n",
    "              float(self.step_count)                  # Current time step\n",
    "              ], dtype=np.float32)\n",
    "\n",
    "\n",
    "        # Điều kiện kết thúc episode\n",
    "        # SOC vượt ngưỡng THẬT SỰ (trước khi clip)\n",
    "        terminated = (self.step_count >= 24)\n",
    "        # Hết bước mà không bị terminated sớm\n",
    "        #truncated = (self.step_count >= 23)\n",
    "\n",
    "        terminated = bool(terminated) # or terminated = True if terminated else False\n",
    "        truncated = bool(truncated)\n",
    "\n",
    "        return self.state, self.reward, terminated,truncated, {}\n",
    "\n",
    "\n",
    "\n",
    "    # Phương thức render: Hiển thị trạng thái hiện tại của môi trường\n",
    "    def render(self):\n",
    "        # In thông báo reset nếu là bước 0\n",
    "        if self.step_count == 0:\n",
    "            print(\"Environment reset. Ready to start.\")\n",
    "        # In thông tin chi tiết cho các bước sau bước 0\n",
    "        elif self.step_count > 0:\n",
    "            # step_idx là index của bước VỪA KẾT THÚC (step_count-1)\n",
    "            step_idx = self.step_count - 1\n",
    "            print(f\"*************************** Step: {step_idx}/{self.max_steps-1} **********************************\")\n",
    "\n",
    "            # Kiểm tra index có hợp lệ với các list dữ liệu đã ghi lại không\n",
    "            if step_idx < len(self.Ge_ar):\n",
    "                print(f\"  Action: Ge={self.Ge_ar[step_idx]:.2f}, Re={self.Re_ar[step_idx]:.2f}, Ebess={self.Be_ar[step_idx]:.2f}\")\n",
    "                # Sửa tên thuộc tính ở đây: dùng self.load_ và self.soc\n",
    "                # Lưu ý: self.ReAvai là array dữ liệu gốc, không phải lịch sử hành động, nên dùng self.ReAvai[step_idx] là đúng.\n",
    "                print(f\"  State: Load={self.load_[step_idx]:.2f}, RE_Avail={self.ReAvai[step_idx]:.2f}, SOC={self.soc[step_idx]*100:.2f}%\")\n",
    "\n",
    "                # Bạn có thể in thêm thông tin Ebess_max/min nếu muốn (cần lưu lại trong info hoặc biến riêng)\n",
    "                # print(f\"  Ebess Limits: [{info['Ebess_min_limit']:.2f}, {info['Ebess_max_limit']:.2f}]\") # Cần lưu info để in\n",
    "\n",
    "                # self.reward là reward của bước hiện tại (vừa tính trong step),\n",
    "                # self.total_reward là tổng reward tích lũy.\n",
    "                # Khi render sau step, bạn in reward của bước vừa xong (step_idx).\n",
    "                # Lưu ý: self.reward được cập nhật ở cuối step, nên khi render sau step_idx, nó đang giữ giá trị reward của step_idx.\n",
    "                print(f\"  Reward: {self.reward:.4f}, Total Reward (so far): {self.total_reward:.4f}\")\n",
    "            else:\n",
    "                # Trường hợp này ít xảy ra nếu render được gọi đúng sau step\n",
    "                print(f\"  Render called at step {self.step_count} but no data logged for step {step_idx}.\")\n",
    "\n",
    "\n",
    "    # Phương thức close: Dọn dẹp tài nguyên (nếu cần)\n",
    "    def close(self):\n",
    "        # Đóng các kết nối, file, v.v. nếu có.\n",
    "        pass # Hiện tại không có tài nguyên cần dọn dẹp đặc biệt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d7edc",
   "metadata": {},
   "source": [
    "## Eval env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa2b28a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_percentage_noise(data, percentage=0.1, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed) # Đặt seed\n",
    "\n",
    "    lower_bound = 1 - percentage\n",
    "    upper_bound = 1 + percentage\n",
    "    factors = np.random.rand(*data.shape) * (upper_bound - lower_bound) + lower_bound\n",
    "    noisy_data = data * factors\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(None) # Đặt lại trạng thái ngẫu nhiên (tùy chọn, nhưng cẩn thận)\n",
    "\n",
    "    return noisy_data\n",
    "\n",
    "class MinimizeProductEnv_eval_24_4(gym.Env):\n",
    "    \"\"\"\n",
    "    Môi trường quản lý năng lượng với dữ liệu load/RE nhiễu.\n",
    "    Khi is_eval_env=True, nó sử dụng bộ dữ liệu cố định cho đánh giá.\n",
    "    \"\"\"\n",
    "    # Thêm is_eval_env và n_eval_episodes vào constructor\n",
    "    def __init__(self, is_eval_env=False, n_eval_episodes=5, noise_percentage=0.1, max_steps=24):\n",
    "        super(MinimizeProductEnv_eval_24_4, self).__init__()\n",
    "\n",
    "        # Cấu hình môi trường\n",
    "        self.max_steps = max_steps\n",
    "        self.load_standard = load_standard.copy() # Copy để tránh sửa dữ liệu global\n",
    "        self.ReAvai_standard = ReAvai_standard.copy()\n",
    "        self.noise_percentage = noise_percentage\n",
    "        self.cap_max = 2000.0\n",
    "        self.P_bess_max = 550.0\n",
    "\n",
    "        # Obs: soc\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([-10, 0, 0, 0], dtype=np.float32),\n",
    "            high=np.array([10, np.inf, np.inf, self.max_steps], dtype=np.float32)\n",
    "        )\n",
    "        # action  Gen, RE, lamda1, lamda2\n",
    "        self.action_space =  spaces.Box(low=np.array([-1,-1], dtype=np.float32),\n",
    "                                     high=np.array([1,1], dtype=np.float32))\n",
    "\n",
    "        self.total_reward = 0\n",
    "        self.Ge_ar = []\n",
    "        self.Re_ar = []\n",
    "        self.Be_ar = []\n",
    "        self.soc = []\n",
    "        self.load_ = []\n",
    "\n",
    "        # Cấu hình cho chế độ đánh giá\n",
    "        self.is_eval_env = is_eval_env\n",
    "        self.n_eval_episodes = n_eval_episodes\n",
    "        self._current_eval_episode = 0 # Theo dõi episode hiện tại trong chu kỳ đánh giá cố định\n",
    "        self._fixed_eval_data = None # Sẽ lưu dữ liệu cố định nếu là env eval\n",
    "\n",
    "        # Nếu instance này là môi trường cho đánh giá, tạo sẵn bộ dữ liệu cố định\n",
    "        if self.is_eval_env:\n",
    "            print(f\"Initializing Evaluation Environment: Generating {n_eval_episodes} fixed noisy data cycles.\")\n",
    "            self._fixed_eval_data = []\n",
    "            # Sử dụng một seed cố định (ví dụ: 42) để đảm bảo bộ 5 dữ liệu này\n",
    "            # giống hệt nhau MỖI KHI script chạy instance này.\n",
    "            initial_eval_seed = 60\n",
    "\n",
    "            for i in range(self.n_eval_episodes):\n",
    "                # Tạo bộ dữ liệu nhiễu cho mỗi episode trong N_EVAL_EPISODES đánh giá\n",
    "                # Sử dụng seed+i để các bộ dữ liệu là khác nhau nhưng vẫn cố định qua các lần chạy\n",
    "                noisy_load = _add_percentage_noise(self.load_standard, self.noise_percentage, seed=initial_eval_seed + i)\n",
    "                noisy_reavai = _add_percentage_noise(self.ReAvai_standard, self.noise_percentage, seed=initial_eval_seed + i + 1000) # Seed khác cho ReAvai\n",
    "\n",
    "                # Lưu bản copy của dữ liệu nhiễu đã tạo\n",
    "                self._fixed_eval_data.append((noisy_load.copy(), noisy_reavai.copy()))\n",
    "            print(\"Fixed evaluation data generated.\")\n",
    "\n",
    "    # Phương thức reset được sửa đổi để chọn nguồn dữ liệu\n",
    "    def reset(self, *, seed=None, options=None, currStep=0, loadForecast=None, REforecast=None, pastRW=0, soc_preset=soc_preset):\n",
    "        # Gọi reset của lớp cha để xử lý seed cho bộ ngẫu nhiên của Gym\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.step_count = currStep\n",
    "\n",
    "        # --- Logic chọn nguồn dữ liệu Load/ReAvai ---\n",
    "        if self.is_eval_env and self._fixed_eval_data is not None:\n",
    "            # Nếu đây là môi trường đánh giá, lấy bộ dữ liệu cố định tương ứng\n",
    "            # self._current_eval_episode sẽ vòng lặp qua 0, 1, 2, 3, 4, 0, 1, ...\n",
    "            load_data_for_episode, re_data_for_episode = self._fixed_eval_data[self._current_eval_episode]\n",
    "            self.load = load_data_for_episode.copy() # Gán dữ liệu đã lưu trữ\n",
    "            self.ReAvai = re_data_for_episode.copy()\n",
    "\n",
    "            # Chuẩn bị cho episode đánh giá tiếp theo trong chu kỳ cố định\n",
    "            self._current_eval_episode = (self._current_eval_episode + 1) % self.n_eval_episodes\n",
    "            # print(f\"Eval Episode Index: {self._current_eval_episode}\") # Có thể in để debug\n",
    "\n",
    "        else:\n",
    "             # Trường hợp này KHÔNG XẢY RA nếu instance được tạo với is_eval_env=True\n",
    "             # nhưng giữ lại để đầy đủ (ví dụ dùng cho training env)\n",
    "            print(\"Warning: Resetting evaluation env without fixed data.\") # Chỉ in nếu có vấn đề\n",
    "            self.load = _add_percentage_noise(self.load_standard, self.noise_percentage, seed=seed)\n",
    "            self.ReAvai = _add_percentage_noise(self.ReAvai_standard, self.noise_percentage, seed=seed + 1 if seed is not None else None)\n",
    "\n",
    "        # --- Khởi tạo các biến episode khác ---\n",
    "        self.Ge_ar = []\n",
    "        self.Re_ar = []\n",
    "        self.Be_ar = []\n",
    "        self.soc = []\n",
    "        self.load_ = []\n",
    "        self.reward = 0 # Reset reward bước hiện tại\n",
    "        self.soc_prev = soc_preset\n",
    "        self.total_reward = pastRW\n",
    "\n",
    "\n",
    "        self.state = np.array([\n",
    "            self.soc_prev,\n",
    "            self.load_standard[currStep],\n",
    "            self.ReAvai_standard[currStep],\n",
    "            float(currStep)\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        # --- Có thể in dữ liệu đầu tiên của episode để kiểm tra ---\n",
    "        # print(f\"Episode Data (Load[0], RE[0]): {self.load[0]:.2f}, {self.ReAvai[0]:.2f}\")\n",
    "        # print(f\"State after reset: {self.state}\")\n",
    "\n",
    "        return self.state, {}\n",
    "\n",
    "    # Phương thức step: Thực hiện một bước trong môi trường\n",
    "    def step(self, action):\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        # Cập nhật trạng thái với action\n",
    "        Egen, Ere = action\n",
    "\n",
    "        _min = 0\n",
    "        _max = self.ReAvai[self.step_count] # Sử dụng giá trị ReAvai đã bị nhiễu cho bước hiện tại\n",
    "\n",
    "        Ere=map_value(Ere,-1,1,_min,_max)\n",
    "        Ebess_max=min(self.P_bess_max,self.soc_prev*self.cap_max)\n",
    "        Ebess_min=max(-self.P_bess_max,-(1-self.soc_prev)*self.cap_max)\n",
    "        Egen= map_value(Egen,-1,1,0,2000) # Sử dụng giá trị load đã bị nhiễu cho bước hiện tại\n",
    "\n",
    "\n",
    "        #tính Ebess\n",
    "        Ebess=self.load[self.step_count]-Egen-Ere\n",
    "        soc = self.soc_prev-Ebess/self.cap_max\n",
    "        reward = -(\n",
    "            Egen\n",
    "            -(Ere-self.ReAvai[self.step_count])\n",
    "            + 1e6 * ( max(0, Ebess - Ebess_max) + max(0, Ebess_min - Ebess) )\n",
    "            )\n",
    "        self.reward = reward\n",
    "        self.soc_prev=soc\n",
    "\n",
    "        self.total_reward += (reward)\n",
    "\n",
    "        self.step_count += 1\n",
    "\n",
    "        if self.step_count >= self.max_steps:\n",
    "            self.state = np.array([\n",
    "            soc,                                     # Updated SOC\n",
    "            0,             # Current load (vẫn là giá trị bị nhiễu của episode này)\n",
    "            0,           # Available renewable energy (vẫn là giá trị bị nhiễu của episode này)\n",
    "            float(self.step_count)                  # Current time step\n",
    "            ], dtype=np.float32)\n",
    "        else:\n",
    "            self.state = np.array([\n",
    "              soc,                                     # Updated SOC\n",
    "              self.load_standard[self.step_count],             # Current load (vẫn là giá trị bị nhiễu của episode này)\n",
    "              self.ReAvai_standard[self.step_count],           # Available renewable energy (vẫn là giá trị bị nhiễu của episode này)\n",
    "              float(self.step_count)                  # Current time step\n",
    "              ], dtype=np.float32)\n",
    "\n",
    "\n",
    "        # Điều kiện kết thúc episode\n",
    "        # SOC vượt ngưỡng THẬT SỰ (trước khi clip)\n",
    "        terminated = (soc > 1.0) or (soc < 0.0)\n",
    "        # Hết bước mà không bị terminated sớm\n",
    "        truncated = (self.step_count >= 24)\n",
    "\n",
    "        terminated = bool(terminated) # or terminated = True if terminated else False\n",
    "        truncated = bool(truncated)\n",
    "\n",
    "\n",
    "        return self.state, self.reward, terminated, truncated, {}\n",
    "\n",
    "    # Phương thức render: Hiển thị trạng thái hiện tại của môi trường\n",
    "    def render(self):\n",
    "        # In thông báo reset nếu là bước 0\n",
    "        if self.step_count == 0:\n",
    "            print(\"Environment reset. Ready to start.\")\n",
    "        # In thông tin chi tiết cho các bước sau bước 0\n",
    "        elif self.step_count > 0:\n",
    "            # step_idx là index của bước VỪA KẾT THÚC (step_count-1)\n",
    "            step_idx = self.step_count - 1\n",
    "            print(f\"*************************** Step: {step_idx}/{self.max_steps-1} **********************************\")\n",
    "\n",
    "            # Kiểm tra index có hợp lệ với các list dữ liệu đã ghi lại không\n",
    "            if step_idx < len(self.Ge_ar):\n",
    "                print(f\"  Action: Ge={self.Ge_ar[step_idx]:.2f}, Re={self.Re_ar[step_idx]:.2f}, Ebess={self.Be_ar[step_idx]:.2f}\")\n",
    "                print(f\"  State: Load={self.load_[step_idx]:.2f}, RE_Avail={self.ReAvai[step_idx]:.2f}, SOC={self.soc[step_idx]*100:.2f}%\")\n",
    "\n",
    "                print(f\"  Reward: {self.reward:.4f}, Total Reward (so far): {self.total_reward:.4f}\")\n",
    "            else:\n",
    "                print(f\"  Render called at step {self.step_count} but no data logged for step {step_idx}.\")\n",
    "\n",
    "\n",
    "    # Phương thức close: Dọn dẹp tài nguyên\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3666275a",
   "metadata": {},
   "source": [
    "# runing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "123a8ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Evaluation Environment: Generating 5 fixed noisy data cycles.\n",
      "Fixed evaluation data generated.\n"
     ]
    }
   ],
   "source": [
    "env = MinimizeProductEnv_24_4()\n",
    "eval_env = MinimizeProductEnv_eval_24_4(is_eval_env=True, n_eval_episodes=5, noise_percentage=0.1, max_steps=24)\n",
    "# real_env = MinimizeProductEnv_real_24_4()\n",
    "check_env(env)\n",
    "check_env(eval_env)\n",
    "# check_env(real_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0694954d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-10.   0.   0.   0.], [10. inf inf 24.], (4,), float32)\n",
      "Box(-1.0, 1.0, (2,), float32)\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.soc_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "267973e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-10.   0.   0.   0.], [10. inf inf 24.], (4,), float32)\n",
      "Box(-1.0, 1.0, (2,), float32)\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "eval_env.reset()\n",
    "print(eval_env.observation_space)\n",
    "print(eval_env.action_space)\n",
    "print(eval_env.soc_prev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0cd479",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf48d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = env\n",
    "eval_env = eval_env\n",
    "eval_env = Monitor(eval_env)\n",
    "\n",
    "# 3. Thiết lập đường dẫn lưu log và mô hình\n",
    "log_dir = os.path.join( \"training_logs_final_env_15_5\")\n",
    "eval_log_path = os.path.join(log_dir, \"eval_logs\")\n",
    "best_model_save_path = os.path.join(log_dir, \"best_model_15_5_soc_0.1\") # EvalCallback sẽ lưu mô hình tốt nhất tại đây\n",
    "\n",
    "os.makedirs(eval_log_path, exist_ok=True)\n",
    "os.makedirs(best_model_save_path, exist_ok=True)\n",
    "\n",
    "# 4. Tạo StopTrainingOnRewardThreshold\n",
    "# Callback này sẽ theo dõi chỉ số \"eval/mean_reward\" được ghi bởi EvalCallback\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=0, verbose=1)\n",
    "\n",
    "\n",
    "# 5. Tạo EvalCallback\n",
    "# Đánh giá mỗi 240 bước thời gian huấn luyện\n",
    "# Chạy 5 episode cho mỗi lần đánh giá\n",
    "# Lưu log đánh giá vào eval_log_path\n",
    "# Lưu mô hình tốt nhất vào best_model_save_path\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    callback_on_new_best=callback_on_best,\n",
    "    best_model_save_path=best_model_save_path,\n",
    "    log_path=eval_log_path,\n",
    "    eval_freq=2064*2,\n",
    "    n_eval_episodes=5,\n",
    "    deterministic=True # Sử dụng chính sách tất định trong khi đánh giá\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f3ffc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1edd9908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KJD1912\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sb3_contrib\\trpo\\trpo.py:154: UserWarning: You have specified a mini-batch size of 120, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2064`, after every 17 untruncated mini-batches, there will be a truncated mini-batch of size 24\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2064 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=4128, episode_reward=-616553910.20 +/- 51983341.98\n",
      "Episode length: 4.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8256, episode_reward=-218036403.79 +/- 115707403.91\n",
      "Episode length: 6.80 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12384, episode_reward=-22608830.31 +/- 10381662.66\n",
      "Episode length: 4.60 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16512, episode_reward=-121504109.42 +/- 24321823.66\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=20640, episode_reward=-104497221.80 +/- 24398119.10\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=24768, episode_reward=-114355376.21 +/- 49477662.27\n",
      "Episode length: 1.80 +/- 0.40\n",
      "Eval num_timesteps=28896, episode_reward=-81930084.23 +/- 70659190.40\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=33024, episode_reward=-72200164.96 +/- 68916866.64\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=37152, episode_reward=-70571359.18 +/- 68430733.49\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=41280, episode_reward=-40437950.14 +/- 21531196.71\n",
      "Episode length: 2.00 +/- 0.00\n",
      "Eval num_timesteps=45408, episode_reward=-42299426.10 +/- 24091757.04\n",
      "Episode length: 2.40 +/- 0.49\n",
      "Eval num_timesteps=49536, episode_reward=-53308666.55 +/- 29137062.73\n",
      "Episode length: 7.20 +/- 1.60\n",
      "Eval num_timesteps=53664, episode_reward=-44304624.51 +/- 47055548.43\n",
      "Episode length: 4.40 +/- 1.96\n",
      "Eval num_timesteps=57792, episode_reward=-58266135.82 +/- 30484826.56\n",
      "Episode length: 9.20 +/- 0.75\n",
      "Eval num_timesteps=61920, episode_reward=-87752902.59 +/- 47888552.65\n",
      "Episode length: 15.40 +/- 0.49\n",
      "Eval num_timesteps=66048, episode_reward=-9530.66 +/- 47.40\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70176, episode_reward=-9119.01 +/- 51.79\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=74304, episode_reward=-9151.55 +/- 47.29\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=78432, episode_reward=-7501.29 +/- 66.03\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=82560, episode_reward=-6519.52 +/- 71.76\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=86688, episode_reward=-6086.28 +/- 66.44\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90816, episode_reward=-5824.13 +/- 66.54\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=94944, episode_reward=-5720.69 +/- 73.56\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=99072, episode_reward=-5257.53 +/- 72.62\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=103200, episode_reward=-5054.12 +/- 65.64\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=107328, episode_reward=-5360.50 +/- 69.05\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=111456, episode_reward=-5593.52 +/- 68.77\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=115584, episode_reward=-5847.29 +/- 59.25\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=119712, episode_reward=-5877.20 +/- 61.04\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=123840, episode_reward=-4772.62 +/- 66.36\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=127968, episode_reward=-5302.32 +/- 68.83\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=132096, episode_reward=-4648.79 +/- 69.36\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=136224, episode_reward=-3452.03 +/- 57.56\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140352, episode_reward=-3319.41 +/- 59.80\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=144480, episode_reward=-3097.66 +/- 59.00\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=148608, episode_reward=-2626.28 +/- 48.90\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=152736, episode_reward=-2372.79 +/- 54.06\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=156864, episode_reward=-2524.25 +/- 54.61\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=160992, episode_reward=-2264.94 +/- 56.76\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=165120, episode_reward=-1984.74 +/- 54.73\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=169248, episode_reward=-1405.79 +/- 62.63\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=173376, episode_reward=-1170.60 +/- 74.36\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=177504, episode_reward=-1051.89 +/- 67.64\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=181632, episode_reward=-968.67 +/- 66.59\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=185760, episode_reward=-885.72 +/- 66.38\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=189888, episode_reward=-963.18 +/- 63.00\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=194016, episode_reward=-837.54 +/- 55.85\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=198144, episode_reward=-843.24 +/- 58.12\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=202272, episode_reward=-812.43 +/- 57.90\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=206400, episode_reward=-725.09 +/- 60.73\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=210528, episode_reward=-605.20 +/- 54.33\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=214656, episode_reward=-715.90 +/- 33.61\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=218784, episode_reward=-635.00 +/- 34.62\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=222912, episode_reward=-694.31 +/- 33.52\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=227040, episode_reward=-702.78 +/- 32.66\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=231168, episode_reward=-699.97 +/- 31.66\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=235296, episode_reward=-734.31 +/- 26.77\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=239424, episode_reward=-752.77 +/- 26.40\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=243552, episode_reward=-771.12 +/- 27.71\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=247680, episode_reward=-705.35 +/- 26.75\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=251808, episode_reward=-625.33 +/- 25.74\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=255936, episode_reward=-630.63 +/- 28.08\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=260064, episode_reward=-627.69 +/- 28.06\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=264192, episode_reward=-626.53 +/- 28.67\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=268320, episode_reward=-632.03 +/- 28.73\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=272448, episode_reward=-613.61 +/- 27.78\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=276576, episode_reward=-611.07 +/- 26.86\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=280704, episode_reward=-611.05 +/- 28.28\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=284832, episode_reward=-530.83 +/- 25.66\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=288960, episode_reward=-533.78 +/- 25.79\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=293088, episode_reward=-524.43 +/- 25.20\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=297216, episode_reward=-521.93 +/- 24.44\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=301344, episode_reward=-545.34 +/- 24.29\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=305472, episode_reward=-619.11 +/- 26.95\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=309600, episode_reward=-630.42 +/- 26.54\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=313728, episode_reward=-635.45 +/- 26.77\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=317856, episode_reward=-567.86 +/- 25.75\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=321984, episode_reward=-592.84 +/- 26.62\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=326112, episode_reward=-587.03 +/- 25.88\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=330240, episode_reward=-578.20 +/- 25.82\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=334368, episode_reward=-592.36 +/- 26.24\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=338496, episode_reward=-594.54 +/- 26.02\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=342624, episode_reward=-594.38 +/- 26.12\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=346752, episode_reward=-511.25 +/- 25.90\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=350880, episode_reward=-500.71 +/- 25.47\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=355008, episode_reward=-607.01 +/- 27.14\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=359136, episode_reward=-544.57 +/- 26.91\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=363264, episode_reward=-554.77 +/- 26.91\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=367392, episode_reward=-549.63 +/- 27.44\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=371520, episode_reward=-541.17 +/- 27.72\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=375648, episode_reward=-545.08 +/- 28.16\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=379776, episode_reward=-594.65 +/- 26.49\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=383904, episode_reward=-520.07 +/- 26.39\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=388032, episode_reward=-602.51 +/- 27.58\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=392160, episode_reward=-520.10 +/- 27.55\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=396288, episode_reward=-515.35 +/- 27.88\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=400416, episode_reward=-600.98 +/- 28.58\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=404544, episode_reward=-543.38 +/- 27.08\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=408672, episode_reward=-542.26 +/- 28.73\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=412800, episode_reward=-548.29 +/- 29.01\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=416928, episode_reward=-560.05 +/- 29.38\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=421056, episode_reward=-585.98 +/- 29.11\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=425184, episode_reward=-589.11 +/- 29.33\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=429312, episode_reward=-521.68 +/- 28.24\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=433440, episode_reward=-552.76 +/- 27.27\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=437568, episode_reward=-558.44 +/- 26.89\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=441696, episode_reward=-562.51 +/- 27.16\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=445824, episode_reward=-554.13 +/- 27.59\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=449952, episode_reward=-545.90 +/- 27.84\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=454080, episode_reward=-475.61 +/- 26.92\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=458208, episode_reward=-552.78 +/- 27.88\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=462336, episode_reward=-555.96 +/- 27.65\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=466464, episode_reward=-580.69 +/- 27.55\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=470592, episode_reward=-505.08 +/- 27.28\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=474720, episode_reward=-513.17 +/- 27.97\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=478848, episode_reward=-591.44 +/- 28.67\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=482976, episode_reward=-594.87 +/- 29.07\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=487104, episode_reward=-578.72 +/- 28.53\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=491232, episode_reward=-503.91 +/- 28.29\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=495360, episode_reward=-583.94 +/- 28.57\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=499488, episode_reward=-512.89 +/- 29.35\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=503616, episode_reward=-513.80 +/- 29.26\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=507744, episode_reward=-516.47 +/- 29.32\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=511872, episode_reward=-523.67 +/- 29.35\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=516000, episode_reward=-450.12 +/- 29.65\n",
      "Episode length: 24.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=520128, episode_reward=-521.81 +/- 29.95\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=524256, episode_reward=-525.27 +/- 29.47\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=528384, episode_reward=-548.24 +/- 29.16\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=532512, episode_reward=-560.76 +/- 28.45\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=536640, episode_reward=-546.34 +/- 28.08\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=540768, episode_reward=-546.98 +/- 28.90\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=544896, episode_reward=-550.70 +/- 29.32\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=549024, episode_reward=-547.23 +/- 29.42\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=553152, episode_reward=-536.91 +/- 29.32\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=557280, episode_reward=-529.67 +/- 29.07\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=561408, episode_reward=-536.35 +/- 29.60\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=565536, episode_reward=-553.88 +/- 30.79\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=569664, episode_reward=-533.92 +/- 31.58\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=573792, episode_reward=-528.95 +/- 31.77\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=577920, episode_reward=-520.75 +/- 34.18\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=582048, episode_reward=-516.74 +/- 34.67\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=586176, episode_reward=-523.94 +/- 33.17\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=590304, episode_reward=-532.11 +/- 38.36\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=594432, episode_reward=-565.06 +/- 36.04\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=598560, episode_reward=-558.61 +/- 35.33\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=602688, episode_reward=-517.04 +/- 33.10\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=606816, episode_reward=-478.43 +/- 31.78\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=610944, episode_reward=-536.58 +/- 30.15\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=615072, episode_reward=-505.39 +/- 27.18\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=619200, episode_reward=-503.28 +/- 27.57\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=623328, episode_reward=-497.28 +/- 28.23\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=627456, episode_reward=-529.83 +/- 27.29\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=631584, episode_reward=-504.45 +/- 33.34\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=635712, episode_reward=-516.15 +/- 33.25\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=639840, episode_reward=-519.96 +/- 30.50\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=643968, episode_reward=-544.70 +/- 28.28\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=648096, episode_reward=-553.79 +/- 24.01\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=652224, episode_reward=-614.78 +/- 25.20\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=656352, episode_reward=-588.44 +/- 25.62\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=660480, episode_reward=-552.78 +/- 29.58\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=664608, episode_reward=-514.60 +/- 29.10\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=668736, episode_reward=-512.02 +/- 27.72\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=672864, episode_reward=-508.09 +/- 26.91\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=676992, episode_reward=-498.97 +/- 30.49\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=681120, episode_reward=-521.21 +/- 28.50\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=685248, episode_reward=-542.49 +/- 28.78\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=689376, episode_reward=-518.04 +/- 28.81\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=693504, episode_reward=-497.22 +/- 30.01\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=697632, episode_reward=-490.97 +/- 28.75\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=701760, episode_reward=-500.01 +/- 29.19\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=705888, episode_reward=-531.01 +/- 27.95\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=710016, episode_reward=-557.09 +/- 28.18\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=714144, episode_reward=-500.05 +/- 28.28\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=718272, episode_reward=-500.35 +/- 28.91\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=722400, episode_reward=-501.26 +/- 27.60\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=726528, episode_reward=-514.82 +/- 27.83\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=730656, episode_reward=-527.11 +/- 26.16\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=734784, episode_reward=-543.82 +/- 27.09\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=738912, episode_reward=-544.67 +/- 28.61\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=743040, episode_reward=-490.72 +/- 30.87\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=747168, episode_reward=-492.41 +/- 32.03\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=751296, episode_reward=-543.71 +/- 29.01\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=755424, episode_reward=-551.97 +/- 28.14\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=759552, episode_reward=-527.32 +/- 29.31\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=763680, episode_reward=-525.88 +/- 33.64\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=767808, episode_reward=-477.64 +/- 34.72\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=771936, episode_reward=-538.72 +/- 35.12\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=776064, episode_reward=-533.05 +/- 35.15\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=780192, episode_reward=-546.21 +/- 34.30\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=784320, episode_reward=-583.33 +/- 29.00\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=788448, episode_reward=-526.13 +/- 28.77\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=792576, episode_reward=-533.35 +/- 29.30\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=796704, episode_reward=-539.42 +/- 29.22\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=800832, episode_reward=-547.08 +/- 29.09\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=804960, episode_reward=-552.80 +/- 29.03\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=809088, episode_reward=-542.30 +/- 29.04\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=813216, episode_reward=-480.74 +/- 29.17\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=817344, episode_reward=-478.75 +/- 29.23\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=821472, episode_reward=-480.72 +/- 29.42\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=825600, episode_reward=-539.88 +/- 29.60\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=829728, episode_reward=-562.09 +/- 29.23\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=833856, episode_reward=-561.87 +/- 29.38\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=837984, episode_reward=-530.88 +/- 29.41\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=842112, episode_reward=-521.19 +/- 29.14\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=846240, episode_reward=-531.08 +/- 29.37\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=850368, episode_reward=-488.98 +/- 27.58\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=854496, episode_reward=-548.93 +/- 28.46\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=858624, episode_reward=-505.04 +/- 26.05\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=862752, episode_reward=-499.61 +/- 27.43\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=866880, episode_reward=-496.13 +/- 28.46\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=871008, episode_reward=-555.07 +/- 28.20\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=875136, episode_reward=-542.71 +/- 27.79\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=879264, episode_reward=-488.25 +/- 28.81\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=883392, episode_reward=-525.87 +/- 28.67\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=887520, episode_reward=-549.10 +/- 29.36\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=891648, episode_reward=-554.71 +/- 29.00\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=895776, episode_reward=-543.71 +/- 37.78\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=899904, episode_reward=-527.64 +/- 50.50\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=904032, episode_reward=-551.79 +/- 51.61\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=908160, episode_reward=-530.14 +/- 47.13\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=912288, episode_reward=-543.57 +/- 42.83\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=916416, episode_reward=-542.23 +/- 36.99\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=920544, episode_reward=-558.45 +/- 44.77\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=924672, episode_reward=-563.02 +/- 50.57\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=928800, episode_reward=-584.90 +/- 43.73\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=932928, episode_reward=-549.06 +/- 49.57\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=937056, episode_reward=-549.52 +/- 31.68\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=941184, episode_reward=-570.42 +/- 27.13\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=945312, episode_reward=-552.33 +/- 25.78\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=949440, episode_reward=-563.91 +/- 25.62\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=953568, episode_reward=-551.21 +/- 26.09\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=957696, episode_reward=-497.43 +/- 24.99\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=961824, episode_reward=-552.88 +/- 24.82\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=965952, episode_reward=-498.08 +/- 20.44\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=970080, episode_reward=-581.18 +/- 21.76\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=974208, episode_reward=-510.36 +/- 18.60\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=978336, episode_reward=-518.93 +/- 20.32\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=982464, episode_reward=-526.16 +/- 21.07\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=986592, episode_reward=-519.38 +/- 20.99\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=990720, episode_reward=-517.61 +/- 21.07\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=994848, episode_reward=-519.02 +/- 21.20\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=998976, episode_reward=-534.26 +/- 19.85\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1003104, episode_reward=-542.16 +/- 21.49\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1007232, episode_reward=-546.88 +/- 24.74\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1011360, episode_reward=-557.82 +/- 22.73\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1015488, episode_reward=-569.82 +/- 25.27\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1019616, episode_reward=-537.20 +/- 23.93\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1023744, episode_reward=-568.66 +/- 24.38\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1027872, episode_reward=-572.35 +/- 24.72\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1032000, episode_reward=-581.05 +/- 24.85\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1036128, episode_reward=-595.68 +/- 24.54\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1040256, episode_reward=-572.26 +/- 21.24\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1044384, episode_reward=-585.93 +/- 22.59\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1048512, episode_reward=-585.56 +/- 23.90\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1052640, episode_reward=-539.33 +/- 23.28\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1056768, episode_reward=-492.86 +/- 19.22\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1060896, episode_reward=-527.07 +/- 21.53\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1065024, episode_reward=-496.77 +/- 22.37\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1069152, episode_reward=-521.11 +/- 22.60\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1073280, episode_reward=-528.43 +/- 22.63\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1077408, episode_reward=-532.44 +/- 21.97\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1081536, episode_reward=-537.35 +/- 21.79\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1085664, episode_reward=-533.77 +/- 19.91\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1089792, episode_reward=-553.60 +/- 20.80\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1093920, episode_reward=-584.29 +/- 23.44\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1098048, episode_reward=-530.30 +/- 23.47\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1102176, episode_reward=-543.40 +/- 26.24\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1106304, episode_reward=-541.03 +/- 25.66\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1110432, episode_reward=-567.17 +/- 26.84\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1114560, episode_reward=-550.20 +/- 37.92\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1118688, episode_reward=-555.19 +/- 28.68\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1122816, episode_reward=-546.47 +/- 42.66\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1126944, episode_reward=-577.73 +/- 39.85\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1131072, episode_reward=-579.86 +/- 48.14\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1135200, episode_reward=-553.51 +/- 48.62\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1139328, episode_reward=-581.94 +/- 47.08\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1143456, episode_reward=-549.64 +/- 40.18\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1147584, episode_reward=-531.44 +/- 45.17\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1151712, episode_reward=-528.61 +/- 43.49\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1155840, episode_reward=-568.61 +/- 29.54\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1159968, episode_reward=-505.19 +/- 33.92\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1164096, episode_reward=-511.80 +/- 33.07\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1168224, episode_reward=-533.86 +/- 36.53\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1172352, episode_reward=-519.06 +/- 36.25\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1176480, episode_reward=-571.66 +/- 28.32\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1180608, episode_reward=-529.61 +/- 34.69\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1184736, episode_reward=-496.04 +/- 41.30\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1188864, episode_reward=-542.43 +/- 33.23\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1192992, episode_reward=-544.92 +/- 25.43\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1197120, episode_reward=-544.36 +/- 23.75\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1201248, episode_reward=-554.93 +/- 24.11\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1205376, episode_reward=-498.31 +/- 30.52\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1209504, episode_reward=-518.65 +/- 26.65\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1213632, episode_reward=-536.75 +/- 29.17\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1217760, episode_reward=-551.03 +/- 27.09\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1221888, episode_reward=-553.27 +/- 27.22\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1226016, episode_reward=-544.85 +/- 27.14\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1230144, episode_reward=-527.07 +/- 25.93\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1234272, episode_reward=-518.66 +/- 25.51\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1238400, episode_reward=-496.63 +/- 27.76\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1242528, episode_reward=-514.63 +/- 28.12\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1246656, episode_reward=-558.35 +/- 28.48\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1250784, episode_reward=-568.64 +/- 28.05\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1254912, episode_reward=-549.72 +/- 25.12\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1259040, episode_reward=-558.94 +/- 24.35\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1263168, episode_reward=-570.32 +/- 21.27\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1267296, episode_reward=-582.32 +/- 22.61\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1271424, episode_reward=-573.92 +/- 20.49\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1275552, episode_reward=-565.70 +/- 22.59\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1279680, episode_reward=-551.09 +/- 23.37\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1283808, episode_reward=-523.64 +/- 24.83\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1287936, episode_reward=-512.46 +/- 24.57\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1292064, episode_reward=-537.09 +/- 25.04\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1296192, episode_reward=-546.65 +/- 26.19\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1300320, episode_reward=-530.38 +/- 24.85\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1304448, episode_reward=-528.28 +/- 22.37\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1308576, episode_reward=-499.37 +/- 23.19\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1312704, episode_reward=-525.87 +/- 26.43\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1316832, episode_reward=-534.25 +/- 28.93\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1320960, episode_reward=-516.49 +/- 27.56\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1325088, episode_reward=-563.79 +/- 25.00\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1329216, episode_reward=-536.86 +/- 28.95\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1333344, episode_reward=-539.87 +/- 24.13\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1337472, episode_reward=-538.09 +/- 24.46\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1341600, episode_reward=-549.68 +/- 26.47\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1345728, episode_reward=-545.46 +/- 23.55\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1349856, episode_reward=-584.92 +/- 23.04\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1353984, episode_reward=-582.58 +/- 24.21\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1358112, episode_reward=-593.91 +/- 22.05\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1362240, episode_reward=-563.22 +/- 22.78\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1366368, episode_reward=-516.98 +/- 18.63\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1370496, episode_reward=-520.86 +/- 21.63\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1374624, episode_reward=-500.72 +/- 27.61\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1378752, episode_reward=-518.82 +/- 31.60\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1382880, episode_reward=-477.92 +/- 28.73\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1387008, episode_reward=-516.34 +/- 28.45\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1391136, episode_reward=-544.21 +/- 26.10\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1395264, episode_reward=-572.50 +/- 24.94\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1399392, episode_reward=-576.40 +/- 24.29\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1403520, episode_reward=-558.41 +/- 24.11\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1407648, episode_reward=-515.28 +/- 22.16\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1411776, episode_reward=-536.96 +/- 22.10\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1415904, episode_reward=-542.78 +/- 20.08\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1420032, episode_reward=-562.36 +/- 19.90\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1424160, episode_reward=-568.49 +/- 20.51\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1428288, episode_reward=-522.19 +/- 22.68\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1432416, episode_reward=-516.35 +/- 23.09\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1436544, episode_reward=-539.04 +/- 23.20\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1440672, episode_reward=-570.82 +/- 22.07\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1444800, episode_reward=-545.27 +/- 19.34\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1448928, episode_reward=-575.42 +/- 17.48\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1453056, episode_reward=-533.00 +/- 18.28\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1457184, episode_reward=-539.00 +/- 19.93\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1461312, episode_reward=-539.30 +/- 20.11\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1465440, episode_reward=-554.18 +/- 20.95\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1469568, episode_reward=-555.98 +/- 22.12\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1473696, episode_reward=-541.86 +/- 21.61\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1477824, episode_reward=-560.87 +/- 21.51\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=1481952, episode_reward=-547.65 +/- 20.51\n",
      "Episode length: 24.00 +/- 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m sb3_trib\u001b[38;5;241m.\u001b[39mTRPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_env, tensorboard_log\u001b[38;5;241m=\u001b[39mlog_dir, n_steps\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2064\u001b[39m, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m120\u001b[39m, gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.995\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      4\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\KJD1912\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sb3_contrib\\trpo\\trpo.py:414\u001b[0m, in \u001b[0;36mTRPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfTRPO,\n\u001b[0;32m    407\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    412\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    413\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfTRPO:\n\u001b[1;32m--> 414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KJD1912\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KJD1912\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:247\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    244\u001b[0m             terminal_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mpredict_values(terminal_obs)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    245\u001b[0m         rewards[idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m terminal_value\n\u001b[1;32m--> 247\u001b[0m \u001b[43mrollout_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_episode_starts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m new_obs  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m dones\n",
      "File \u001b[1;32mc:\\Users\\KJD1912\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:474\u001b[0m, in \u001b[0;36mRolloutBuffer.add\u001b[1;34m(self, obs, action, reward, episode_start, value, log_prob)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(reward)\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_starts[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(episode_start)\n\u001b[1;32m--> 474\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_probs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m log_prob\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = sb3_trib.TRPO(\"MlpPolicy\", train_env, tensorboard_log=log_dir, n_steps= 2064, batch_size = 120, gamma = 0.995, verbose = 0)\n",
    "model.learn(total_timesteps=24*100000, callback=eval_callback, progress_bar=False)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Thời gian chạy: {elapsed_time} giây\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "692e0084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 6428), started 0:00:35 ago. (Use '!kill 6428' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c42cf28cbd2f74b2\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c42cf28cbd2f74b2\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=\"training_logs_final_env_15_5\"\n",
    "%reload_ext tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
